#ifndef HASH_TABLE_HPP
#define HASH_TABLE_HPP

#include <cstddef>
#include <cstdint>
#include <memory>
#include <functional>
#include <type_traits>
#include <vector>
#include <optional>
#include <bit>
#include <cassert>
#include <bitset>
#include <mutex>
#include <shared_mutex>
// To-Do: 
/*
1. Testing each Method - likely some inconsistencies in passing by ref/ptr (particularly in insert) 
2. Multithreading/Concurrency applications & guardrails - MAJOR undertaking probably. Let's see if this shit runs first
*/

// FNV-1a Hashing Algorithm with SIMD.
// *** Some SIMD - Integrate with NEON later if you want. ***


[[nodiscard]] inline std::uint64_t hash_key(const void* data, size_t len) noexcept {
    constexpr uint64_t INITIAL = 0xCBF29CE484222325;
    constexpr uint64_t MULTIPLIER = 0x100000001B3;

    uint64_t hash = INITIAL;
    const uint8_t* bytes = static_cast<const uint8_t*>(data);
    size_t i = 0;

    for (; i + 4 <= len; i += 4) { 
        hash = (hash ^ bytes[i]) * MULTIPLIER;
        hash = (hash ^ bytes[i + 1]) * MULTIPLIER;
        hash = (hash ^ bytes[i + 2]) * MULTIPLIER;
        hash = (hash ^ bytes[i + 3]) * MULTIPLIER;
    }

    for (; i < len; i++) {
        hash = (hash ^ bytes[i]) * MULTIPLIER;
    }

    return hash;
}
template <typename K>
[[nodiscard]] inline std::uint64_t hash_key(const K& key) noexcept {
    if constexpr (std::is_integral_v<K>) { 
        return hash_key(reinterpret_cast<const uint8_t*>(&key), sizeof(K));  
    } else if constexpr (std::is_same_v<K, std::string> || std::is_same_v<K, std::string_view>) {
        return hash_key(reinterpret_cast<const uint8_t*>(key.data()), key.size());
    } else if constexpr (std::is_same_v<K, std::vector<uint8_t>>) {
        return hash_key(reinterpret_cast<const uint8_t*>(key.data()), key.size());
    } else {
        static_assert(sizeof(K) == 0, "unsupported key type for hash_key :(.");
    }
}

// Basic RAII - Delete copy-consructors, allow transfer of ownership only.
// Our Node contains its HashCode (generated by FNV-1a) and next_ (pointing to a unique_ptr).

template<typename K, typename V>
class HNode {
public:
    HNode(K key, V value, uint64_t hash) : key_(std::move(key)), value_(std::move(value)), hcode_(hash) {}

    HNode() = delete; 
    ~HNode() = default; 
    
    HNode(const HNode&) = delete; 
    HNode& operator=(const HNode&) = delete; 
    
    HNode(HNode&&) noexcept = default; 
    HNode& operator=(HNode&&) noexcept = default; 

    K key_;
    V value_;
    std::uint64_t hcode_;
    std::unique_ptr<HNode<K, V>> next_;
};

template<typename K, typename V>
class HTable {
public:
    // Constructor to roof initial_size to the nearest exponent of 2 and call initialize. (in private)
    explicit HTable(size_t initial_size = 0) {
        if (initial_size > 0) {
            initialize(std::bit_ceil(initial_size));
        }
    }
    // RAII - Delete Copy Construction & Allow ownership transfer of unique_pointer nodes
    HTable(const HTable&) = delete; 
    HTable& operator=(const HTable&) = delete; 

    HTable(HTable&&) noexcept = default; 
    HTable& operator=(HTable&&) noexcept = default; 

    // Insert function with safety check (initialize on k_min_cap of 4 if empty) and move semantics.
    void insert(K key, V value) {
        if (buckets_.empty()) {
            initialize(k_min_cap);
        }
    
        uint64_t hash = hash_key(key);  // FIXED: Use correct `hash_key` function
        size_t pos = hash & mask_;
        std::cout << "[Insert] Key: " << key << ", Hash: " << hash << ", pos: " << pos << std::endl;
        std::unique_lock lock(*bucket_locks_[pos]);          
        auto node = std::make_unique<HNode<K, V>>(std::move(key), std::move(value), hash);
        node->next_ = std::move(buckets_[pos]);          
        buckets_[pos] = std::move(node);
        size_++;
    }
    

    HNode<K, V>* lookup(const K& key) {
        if (buckets_.empty()) return nullptr;
    
        uint64_t hash = hash_key(key);
        size_t pos = hash & mask_;
    
        std::shared_lock lock(*bucket_locks_[pos]);  // shared access for readers
        HNode<K, V>* current = buckets_[pos].get();
    
        while (current) {
            if (current->key_ == key) return current;
            current = current->next_.get();
        }
        return nullptr;
    }
    
    

    // Accept anonymous comparator function and key, given the mask_ and position, get index current with raw ptr
    // and iterate until comparator is true, we return the unique_pointer after rechaining (avoid dangling ptrs).
    std::optional<V> remove(const K& key) {
        if (buckets_.empty()) {
            return std::nullopt;
        }
    
        uint64_t hash = hash_key(key);
        size_t pos = hash & mask_;
        std::unique_lock lock(*bucket_locks_[pos]);  
        HNode<K, V>* current = buckets_[pos].get();
        HNode<K, V>* prev = nullptr;
    
        while (current) {
            if (current->key_ == key) {
                std::optional<V> value = std::move(current->value_);
    
                if (prev) {
                    prev->next_ = std::move(current->next_);
                } else {
                    buckets_[pos] = std::move(current->next_);
                }
    
                size_--;
                return value;
            }
            prev = current;
            current = current->next_.get();
        }
        return std::nullopt;
    }

    std::unique_ptr<HNode<K, V>> steal_first_node(size_t& pos) {
        while (pos < buckets_.size() && !buckets_[pos]) {
            pos++; 
        }
    
        if (pos >= buckets_.size()) {
            return nullptr;
        }

        std::unique_lock lock(*bucket_locks_[pos]);  
        auto node = std::move(buckets_[pos]);
        buckets_[pos] = std::move(node->next_);
        size_--;
        return node;
    }
    
    [[nodiscard]] size_t size() const noexcept { return size_; } 
    [[nodiscard]] size_t capacity() const noexcept { return mask_ + 1; } 
    bool empty() const noexcept { return size_ == 0; } 

    void clear() {
        buckets_.clear();  
        size_ = 0;
        mask_ = 0;
    }
    
private:
    std::vector<std::unique_ptr<HNode<K,V>>> buckets_; 
    std::vector<std::unique_ptr<std::shared_mutex>> bucket_locks_;
    size_t mask_{0}; 
    size_t size_{0}; 

    static constexpr size_t k_min_cap = 4;

    // Helper function for our constructor, 
    // A. Check if capacity is exponent of 2, if true, continue
    // B. Ensure capacity is at least 4, and resize it
    // C. Initilize mask_ as capacity - 1 (bitmasking operation for XOR replacement) and init size_ to 0. 
    void initialize(size_t capacity) { 
        assert(std::has_single_bit(capacity));  
        capacity = std::max(capacity, k_min_cap); 
        
        buckets_.resize(capacity);
        bucket_locks_.resize(capacity);
        for (size_t i = 0; i < capacity; ++i) {
            bucket_locks_[i] = std::make_unique<std::shared_mutex>();
        }

        mask_ = capacity - 1; 
        size_ = 0; 
    }
};

template<typename K, typename V>
class HMap {
public:
    HMap() = default;
    ~HMap() = default;

    // RAII - Delete Copy Construction & Allow ownership transfer of unique_pointer nodes
    HMap(const HMap&) = delete; 
    HMap& operator=(const HMap&) = delete; 

    HMap(HMap&& other) noexcept
    : primary_table_(std::move(other.primary_table_)),
      temporary_table_(std::move(other.temporary_table_)),
      resizing_pos_(other.resizing_pos_) {}

    HMap& operator=(HMap&& other) noexcept {
        if (this != &other) {
            primary_table_ = std::move(other.primary_table_);
            temporary_table_ = std::move(other.temporary_table_);
            resizing_pos_ = other.resizing_pos_;
        }
        return *this;
    }

    // If our primary HTable is empty, we 
    void insert(K key, V value) { 
        std::unique_lock lock(map_mutex_);
        if (primary_table_.empty()) {
            primary_table_ = HTable<K, V>(k_min_cap);
        }
        uint64_t hash = hash_key(key);
    
        // Calculate the load factor using floating point division
        double load_factor = static_cast<double>(primary_table_.size()) / primary_table_.capacity();
        std::cout << "[Insert] Load factor: " << load_factor << " for key " << key << std::endl;
        
        // If the load factor exceeds the threshold, trigger resizing
        if (load_factor >= k_max_load_factor) {
            std::cout << "[Resize Triggered] Load factor exceeded threshold." << std::endl;
            start_resize();
        }
    
        primary_table_.insert(key, value);
        lock.unlock();
        help_resize();
    }
    
    
    
    
    
    // Pass in a key and comparator function. We call help_resize before to ensure our node isn't lost after the 15 swaps!
    V* find(const K& key) {
        help_resize();
    
        {
            std::shared_lock lock(map_mutex_);  
            
            if (auto* node = primary_table_.lookup(key)) {
                return &(node->value_);
            }
        }
    
        if (temporary_table_) {  
            if (auto* node = temporary_table_->lookup(key)) {
                return &(node->value_);
            }
        }
    
        return nullptr;
    }
    
    // Once again, similar to find - We pass in a key and comparator function, and we call help_resize. We should call it earlier here as 
    // this improves our performance as keys are migrated to primary_DB, this increases our chances of finding the key in the first check.
    std::optional<V> remove(const K& key) {
        help_resize();

        std::unique_lock lock(map_mutex_);  // Lock for modifying structure
        if (auto node = primary_table_.remove(key)) {  // node is `std::optional<V>`
            return node;  // FIXED: return `std::optional<V>` directly
        }
    
        if (temporary_table_) {
            if (auto node = temporary_table_->remove(key)) {
                return node;  // FIXED
            }
        }
    
        return std::nullopt;
    }
    
    
    
    std::unique_ptr<HNode<K, V>> steal_first_node(size_t& pos) {
        std::cout << "[Steal] pos=" << pos << std::endl;
        std::unique_lock lock(map_mutex_);
        if (!temporary_table_) return nullptr;
        return temporary_table_->steal_first_node(pos);
    }
    

    
    [[nodiscard]] size_t size() const noexcept {
        return primary_table_.size() + 
               (temporary_table_ ? temporary_table_->size() : 0); // Get the size of both primary and resizing table (if exists).
    }
    
    bool empty() const noexcept { return size() == 0; } // method for checking if the above size()==0, for convenience.
    
    void clear() {
        primary_table_.clear();  // Clear primary hash table
        temporary_table_.reset();  // Remove temporary table if resizing
        resizing_pos_ = 0;  // Reset migration position
    }
    
private:
    static constexpr size_t max_work = 15; // 15 transfers per help_resize call 
    static constexpr size_t k_max_load_factor = 8; // Max Average of 8 nodes per bucket before start_resize is called!
    static constexpr size_t k_min_cap = 4; // If constructor is called we want the minimum capacity to be 4 buckets!

    HTable<K,V> primary_table_;
    std::optional<HTable<K,V>> temporary_table_;
    size_t resizing_pos_{0};
    mutable std::shared_mutex map_mutex_;
    void help_resize() {
        // Sanity check! Do not proceed if resizing table does not exist
        if (!temporary_table_) {
            std::cout << "[HelpResize] No temporary table. Skipping." << std::endl;
            return;
        }
    
        std::cout << "[Resize] Starting. pos=" << resizing_pos_ << ", size=" 
                  << temporary_table_->size() << std::endl; 
    
        std::unique_lock lock(map_mutex_); 
        size_t work_done = 0;
    
        while (work_done < max_work && !temporary_table_->empty()) {
            std::cout << "[HelpResize] Attempting steal @ pos " << resizing_pos_ << std::endl;
    
            auto node = temporary_table_->steal_first_node(resizing_pos_);
            if (node) {
                std::cout << "[HelpResize] Moving key: " << node->key_ << std::endl;
                primary_table_.insert(std::move(node->key_), std::move(node->value_));
                work_done++;
            } else {
                std::cout << "[HelpResize] No node to steal at pos " << resizing_pos_ << ", incrementing." << std::endl;
                resizing_pos_++;  // Only advance when the bucket is empty
            }
    
            // Debug if resizing_pos_ goes out of bounds
            if (resizing_pos_ >= temporary_table_->capacity()) {
                std::cout << "[HelpResize] Warning: resizing_pos_ exceeded capacity. Resetting." << std::endl;
                resizing_pos_ = 0;  // Reset if out of bounds
            }
        }
    
        if (temporary_table_->empty()) { 
            temporary_table_.reset();
            resizing_pos_ = 0;
            std::cout << "[HelpResize] Completed resizing, reset temporary_table." << std::endl;
        }
    }
    
    void start_resize() {
        std::unique_lock lock(map_mutex_);
        assert(!temporary_table_); // First a sanity check that resizing table doesn't already exist!
    
        size_t new_capacity = primary_table_.capacity() * 2; // New capacity is 2x previous, this is a standard approach.
        std::cout << "[Resize Start] New cap = " << new_capacity << std::endl;
    
        // Move the existing primary table's data to temporary table for resizing
        temporary_table_.emplace(std::move(primary_table_)); 
    
        // Create a new primary table with the new doubled capacity
        primary_table_ = HTable<K,V>(new_capacity); 
    
        resizing_pos_ = 0; // Set the position where we start migrating data from temporary table to primary table.
    }
    
    
};

#endif // HASH_TABLE_HPP